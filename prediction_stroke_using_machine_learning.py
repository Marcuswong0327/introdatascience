import streamlit as st

st.title("Stroke Prediction Using Machine Learning")

st.write("This is a simple Streamlit app that predicts stroke risk.")


# -*- coding: utf-8 -*-
"""Prediction Stroke Using Machine Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14SwSaISYKh2KofOLLHf2VzzWcghHLtM3
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import make_scorer
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as imbPipeline


import xgboost as xgb
import lightgbm as lgb

# Ignore warning messages
import warnings
warnings.filterwarnings('ignore')

"""# Load Dataset"""

raw_stroke_df = pd.read_csv('healthcare-dataset-stroke-data.csv')
#Check the head of dataset
raw_stroke_df.head()

raw_stroke_df.shape

raw_stroke_df.isna().sum()

raw_stroke_df.info()

raw_stroke_df.describe()

for column in raw_stroke_df.columns:
    if raw_stroke_df[column].dtype in ['int64', 'float64']:
        # Plot histograms for numerical columns
        plt.figure(figsize=(8, 6))
        sns.histplot(data=raw_stroke_df, x=column, kde=True)
        plt.title(f'Distribution of {column}')
        plt.xlabel(column)
        plt.ylabel('Frequency')
        plt.show()
    elif raw_stroke_df[column].dtype == 'object':
        # Plot counts for categorical columns
        plt.figure(figsize=(8, 6))
        sns.countplot(data=raw_stroke_df, x=column)
        plt.title(f'Count of {column}')
        plt.xlabel(column)
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.show()

"""# After the pre-data visualisation (just to see how the data are distributed)


For age, there are even an age of 0

Smoking history even up to 4 categories

For bmi have up to almost 100

therefore, we decided to further dive into their numeric and string values, to further understand.
"""

# Calculate the interquartile range (IQR) for 'bmi'
Q1 = raw_stroke_df['bmi'].quantile(0.25)
Q3 = raw_stroke_df['bmi'].quantile(0.75)
IQR = Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = raw_stroke_df[(raw_stroke_df['bmi'] < lower_bound) | (raw_stroke_df['bmi'] > upper_bound)]

# Count the number of outliers
num_outliers = len(outliers)

print("Number of outliers in 'bmi' column:", num_outliers)

# Access the 'bmi' values of the outliers DataFrame
outlier_values = outliers['bmi']

# Display the minimum and maximum values of the outliers
min_outlier_value = outlier_values.min()
max_outlier_value = outlier_values.max()

print("Lower bound:", lower_bound)
print("Upper bound:", upper_bound)

print("Minimum outlier value in 'bmi' column:", min_outlier_value)
print("Maximum outlier value in 'bmi' column:", max_outlier_value)

# The minimum is still under acceptable range, therefore we accept it but not for the maximum range

# Filter the DataFrame for records below the lower bound
below_lower_bound = raw_stroke_df[raw_stroke_df['bmi'] < lower_bound]

# Count the number of records below the lower bound
num_below_lower_bound = len(below_lower_bound)

print("Number of records below the lower bound in 'bmi' column (outliers):", num_below_lower_bound)
print(lower_bound)

"""### Unique values"""

def get_unique_values(dataframe,col_list):
    for col in col_list:
        print('Column:',col)
        print('Unique values:',dataframe[col].unique())
        print('\n')

get_unique_values(raw_stroke_df,['gender','hypertension','heart_disease','ever_married','work_type','Residence_type','smoking_status','stroke'])

# Assuming raw_stroke_df is your dataframe containing the data
# Get unique values of smoking_history
unique_smoking_history = raw_stroke_df['smoking_status'].unique()

# Iterate over unique values and count records associated with each value
for smoking_status in unique_smoking_history:
    count = raw_stroke_df[raw_stroke_df['smoking_status'] == smoking_status].shape[0]
    print(f"Number of records with smoking history '{smoking_status}': {count}")



"""# Data Preprocessing and Data Cleaning

Check

Data Cleaning

Removal of Duplicate rows (if any)
"""

raw_stroke_df.duplicated().sum()


raw_stroke_df.drop_duplicates(inplace=True)


raw_stroke_df.duplicated().sum()


raw_stroke_df.isna().sum()

raw_stroke_df.describe(include='object')

raw_stroke_df.describe()

"""# Filter age that below 5 years old"""

# Round the age values to integers
raw_stroke_df['age'] = raw_stroke_df['age'].round().astype(int)

# Filter out rows where age is less than 5
raw_stroke_df = raw_stroke_df[raw_stroke_df['age'] >= 5]

raw_stroke_df.describe()

"""Selecting an age threshold of 5 ensures our analysis focuses on individuals beyond early childhood,
where stroke onset is rare.

By targeting this demographic, we obtain more accurate insights for guiding interventions
to manage the condition effectively.

# Convert gender into numerical
"""

# Convert gender values to 0 for male and 1 for female
raw_stroke_df['gender'] = raw_stroke_df['gender'].apply(lambda x: 1 if x.lower() == 'female' else 0)

print(raw_stroke_df['gender'])

raw_stroke_df

"""# Outlier Handling for the 'bmi' Column"""

# Replace BMI values greater than 70 with 70
raw_stroke_df.loc[raw_stroke_df['bmi'] > 70, 'bmi'] = 70

# Display the DataFrame
raw_stroke_df['bmi'].describe()

"""In Malaysia, BMI values above 70 are incredibly rare.


By setting a cap at 70, we're keeping our health assessments grounded in the reality of our community's health.

This approach helps us focus on what's most common and relevant,

making sure we're accurately assessing health and offering the right support where it's needed most.
"""

###Remove missing value in bmi column using mean

raw_stroke_df['bmi'].fillna(raw_stroke_df['bmi'].mean(), inplace=True)

"""# Smoke History Column

Since value of "No Info" makes no sense, therefore we decided to drop it. Then, to make the data simpler and clearer, we decided to categorized it as smoker and non-smoker. We treat all the categories like "smokes, formerly smoked" as smokers, and only "never" as non-smoker.
"""

# Step 1: Drop records with 'No Info' smoking history
raw_stroke_df = raw_stroke_df[raw_stroke_df['smoking_status'] != 'Unknown']

raw_stroke_df['smoking_status'].unique()


# Step 2: Categorize smokers and non-smokers
raw_stroke_df['smoking_status'] = raw_stroke_df['smoking_status'].apply(
    lambda x: 0 if x == 'never smoked' else (1 if x in ['formerly smoked', 'smokes'] else np.nan)
)
raw_stroke_df['smoking_status'].fillna(0, inplace=True)  # replace to 'never smoked'



# Display the updated dataframe
print(raw_stroke_df.head())

"""# Data Preprocessing

# Scaling Numeric Features using Min-Max Scaling (Done)

Features with larger scales may dominate the learning process of many machine learning algorithms. Scaling ensures that all features contribute equally to the model, preventing features with larger scales from having a disproportionate influence on the outcome.
"""

scaler = MinMaxScaler(feature_range=(0, 1))

bmi_scaler = MinMaxScaler()
age_scaler = MinMaxScaler()

raw_stroke_df['scaled_bmi'] = bmi_scaler.fit_transform(raw_stroke_df['bmi'].values.reshape(-1, 1)).flatten()
raw_stroke_df['scaled_age'] = age_scaler.fit_transform(raw_stroke_df['age'].values.reshape(-1, 1)).flatten()

# Display the resulting DataFrame
raw_stroke_df

# Define columns to be scaled
columns_to_scale = ['avg_glucose_level']

# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Scale the selected columns and create new columns with scaled values
for column in columns_to_scale:
    # Create new column name
    new_column_name = f"scaled_{column}"
    # Scale the column and assign to the new column
    raw_stroke_df[new_column_name] = scaler.fit_transform(raw_stroke_df[[column]])

# Display the updated dataframe with scaled columns
print(raw_stroke_df)

# List of columns to include in the filtered dataframe
columns_to_include = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type','Residence_type','stroke', 'smoking_status', 'scaled_bmi', 'scaled_age', 'scaled_avg_glucose_level']

# Initialize an empty dataframe
filtered_stroke_df = pd.DataFrame()

# Iterate over each column to determine whether to use scaled or original version
for column in columns_to_include:
    # Check if a scaled version of the column exists
    if f"scaled_{column}" in raw_stroke_df.columns:
        # Use the scaled version if available
        filtered_stroke_df[column] = raw_stroke_df[f"scaled_{column}"]
    else:
        # Use the original version if scaled version does not exist
        filtered_stroke_df[column] = raw_stroke_df[column]

# Display the filtered dataframe
print(filtered_stroke_df)

"""# Data Analysis

Analysis and visualization after preprocessing
"""

filtered_stroke_df.describe().round(2)



"""# Histogram for scaled age"""

plt.hist(filtered_stroke_df['scaled_age'], bins=25, edgecolor='black', alpha=0.7, label='Histogram', density=True)
sns.kdeplot(filtered_stroke_df['scaled_age'], color='red', label='Density')
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency/Density')
plt.legend()
plt.show()

"""Distribution plot for BMI and average glucose level"""

for col in ['scaled_bmi','scaled_avg_glucose_level']:
    sns.distplot(filtered_stroke_df[col], bins=25)
    plt.title(f'{col} Distribution')
    plt.xlabel(f'{col}')
    plt.show()

"""# Counting binary variables(done editing)

"""

for col in ['gender', 'hypertension', 'heart_disease', 'stroke', 'smoking_status','ever_married', 'work_type', 'Residence_type']:
    sns.countplot(x=col, data=filtered_stroke_df)
    plt.title(f'{col} Distribution')
    plt.show()
# Gender: 0=Male, 1=Female

raw_stroke_df['stroke'].value_counts()

"""## Pair plot for numeric features

### Hooray! the pair plot looks good!
"""

from sklearn.utils import resample

df_majority = raw_stroke_df[raw_stroke_df.stroke == 0]
df_minority = raw_stroke_df[raw_stroke_df.stroke == 1]

df_minority_upsampled = resample(df_minority,
                                 replace=True,
                                 n_samples=2000,
                                 random_state=42)

balanced_df = pd.concat([df_majority, df_minority_upsampled])

# Shuffle the final dataset (optional but recommended)
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

sns.pairplot(balanced_df, hue='stroke')

"""
##  Countplot of gender, hypertension, heart disease, smoking status vs stroke"""

for col in ['gender', 'hypertension', 'heart_disease', 'smoking_status', 'ever_married', 'work_type', 'Residence_type']:
    sns.countplot(x=col, data=balanced_df, hue='stroke')
    plt.title(f'{col} Distribution')
    plt.show()

    rates = filtered_stroke_df.groupby(col)['stroke'].mean() * 100
    formatted_rates = rates.apply(lambda x: "{:.2f}".format(x))
    print(f"Rates of stroke occurrence for different {col} categories:")
    print(formatted_rates)

"""# (Double Check, should be fine) Analysis between categorical variables with stroke

Gender vs Stroke: Both gender have almost the same rate of having stroke.

Hypertension vs Stroke: Patients with hypertension has a higher rate (13.39) of having stroke.

Heart Disease vs Stroke: Patients with heart disease has a higher rate (17.11) of having stroke.

Smoking Status vs Stroke: Both status are having almost similar rate of having hypothesis, but there is a slight difference. Non-smoker has lower rate (4.76) to have stroke compared to smoker (6.69).

Marital status vs Stroke: Married people have higher rate of developing stroke which is 6.68 compared to non-married people which is only 2.45

Work Type vs Stroke: People who work as self-employed has rate of 8.60 followed by private sector which accounts for 5.25 followed by Govern_job which accounts for 4.67 and 0 for never_worked and children

Residence type vs Stroke: Almost same probability to develop stroke. People who live in rural has rate of 5.48 chances whereas people who live in urban has slightly higher rate which is 5.84.

In short, patients with hypertension and heart disease are more likely to develop stroke than those without.

# BoxPlot of BMI, age, avg glucose level vs stroke classification
"""

for col in ['scaled_bmi', 'scaled_age', 'scaled_avg_glucose_level']:
    sns.boxplot(x='stroke', y=col, data=balanced_df, hue='stroke')
    plt.title(f'{col} vs Stroke')
    plt.show()

"""#### (Double check, should be fine) Analysis between continuous variables with stroke

Scaled BMI vs Stroke:
Boxplot shows that the scaled BMI contains outlier for both stroke patients and non-stroke patients.
Most of the non-stroke patients have the scaled BMI lied between 0.2-0.4 where most of the stroke patients have the scaled BMI lied between 0.3-0.4. According to boxplot, non-stroke patients have a larger distribution of scaled BMI. However, the variable are less effective since it does not affect the results much but still taken into consideration.

Scaled Age vs Stroke: Boxplot shows that the scaled age for stroke patients contain outlier. Most of the non-stroke patients have the scaled age lied between 0.3-0.7, while most of the stroke patients have the scaled age lied between 0.7-0.9, indicates that most of the stroke patients are from higher ages. The variable is effective.


Scaled Blood Glucose Level vs Stroke: Boxplot shows that the scaled blood glucose level for stroke contains outlier. Most of the non-stroke patients have the scaled blood glucose level lied between 0.1-0.3, while most of the diabetes patients have the scaled blood glucose level lied between 0.1-0.7, indicates that most of the stroke patients have higher blood glucose level. The variable is effective.

# Scatterplot Age vs BMI colored by stroke classification

Compare 2 Continuous variables with stroke
"""

sns.scatterplot(x='scaled_age', y='scaled_bmi', hue='stroke', data=balanced_df)
plt.title('Age vs BMI')
plt.show()

"""##### Orange colour indicates stroke patients whereas blue colour indicates non-stroke patients.
##### Most of the stroke patients distributed at a higher scaled age
##### However, there is not much different of distribution of stroke patients between high scaled bmi and low scaled BMI (almost equally distributed), indicating that the effect of BMI on stroke is less obvious.
##### In short, stroke patients are mostly affected by age rather than BMI.

# Violin plot of BMI against stroke classification split by gender
"""

# Gender: 0=Male, 1=Female
sns.violinplot(x='stroke', y='scaled_bmi', hue='gender', split=True, data=balanced_df)
plt.title('BMI vs Stroke split by Gender')
plt.show()

"""##### (Double check, should be fine) Kernel Density Estimation (KDE) plot shows the probability density function of the data.

For non-stroke patients, both males and females have a scaled BMI distribution that is approximately symmetrical with a median around 0.3.

For stroke patients, both males and females have a scaled BMI distribution with a median around 0.3. However, the distribution for stroke patients is asymmetrical and positively skewed, indicating a higher proportion of individuals with higher scaled BMI values compared to the non-stroke group, even though the medians are similar.

From the violin plot above, we can conclude that factor of gender do not affect on the rate of having stroke while BMI slightly affect the rate.

# Interaction between variables and stroke
"""

# Interaction between gender, BMI and diabetes
sns.boxplot(x='stroke', y='scaled_bmi', hue='gender', data=filtered_stroke_df)
plt.title('BMI Distribution by Stroke Status and Gender')
plt.show()

"""# BMI are slightly higher for stroke patients
# Gender does not show much effect
"""

# Interaction between gender, Age and diabetes
sns.boxplot(x='stroke', y='scaled_age', hue='gender', data=filtered_stroke_df)
plt.title('Age Distribution by Stroke Status and Gender')
plt.show()

"""# Age shows obvious effect on the rate of having stroke, that is higher age have a higher chance of having stroke.

# Encoding (Got logic problem in using data)

### I noticed that after we use resample data, suppose we need to use the resampling data for the following code. But the resampling data seems like havent filtered and cleaned yet, because later in the random forest modelling will face ValueError problem.

must check again the logic for encoding part especially when to use balanced_df and filtered_stroke_df.

One-hot encoding is performed to prepare categorical variables for inclusion in models, ensuring the models can effectively utilize all available information without introducing biases or misinterpretations.
 One-hot encoding is performed on the 'gender' and 'smoking_status' variables in the stroke dataset
"""

data = balanced_df.copy()

def perform_one_hot_encoding(balanced_df, column_name):
    # Perform one-hot encoding on the specified column
    dummies = pd.get_dummies(balanced_df[column_name], prefix=column_name)

    # Drop the original column and append the new dummy columns to the dataframe
    balanced_df = pd.concat([balanced_df.drop(column_name, axis=1), dummies], axis=1)

    return balanced_df

# Perform one-hot encoding on the gender variable
data = perform_one_hot_encoding(data, 'gender')

# Perform one-hot encoding on the smoking history variable
data = perform_one_hot_encoding(data, 'smoking_status')

"""# Correlation Matrix ( cannot work after using resampling data)

To show and compare relationship between all variables.
"""

# 找出所有非数值型的 columns
non_numeric_columns = data.select_dtypes(exclude=['number']).columns

print("Non-numeric columns:")
print(non_numeric_columns)

print(data[['gender_0', 'gender_1', 'smoking_status_0', 'smoking_status_1']].head())

bool_columns = ['gender_0', 'gender_1', 'smoking_status_0', 'smoking_status_1']
data[bool_columns] = data[bool_columns].astype(int)

data['ever_married'] = data['ever_married'].map({'Yes': 1, 'No': 0})
data['Residence_type'] = data['Residence_type'].map({'Urban': 1, 'Rural': 0})
# One-hot encode work_type
work_type_dummies = pd.get_dummies(data['work_type'], prefix='work_type')

# Drop the original column and add the dummies
data = pd.concat([data.drop('work_type', axis=1), work_type_dummies], axis=1)

# Compute the correlation matrix
correlation_matrix = data.corr()
# HeatMap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='Spectral', linewidths=0.5, fmt='.2f')
plt.title("Correlation Matrix Heatmap")
plt.show()

"""HeatMap is targeted to the variables that compare with the stroke to show their relationship."""

# Targeted heatmap
# Create a heatmap of the correlations with the target column
corr = data.corr()
target_corr = corr['stroke'].drop('stroke')

# Sort correlation values in descending order
target_corr_sorted = target_corr.sort_values(ascending=False)

sns.heatmap(target_corr_sorted.to_frame(), cmap="Spectral", annot=True, fmt='.2f')
plt.title('Correlation with Stroke')
plt.show()

"""As the HeatMap shown, the effect of variables on stroke are list accordingly from the most important to less important. Therefore, we can conclude that the most important variables are themost reliable factors to consider as the stroke prediction.

The most important factors to consider are age, hypertension, heart disease, average glucose level.

Smoking status, Gender, Residence Type are not taken into consideration as the factor does not show effect on determining the stroke patient.

# Handling class imbalance
"""

# Count plot for the 'stroke' variable
sns.countplot(x='stroke', data=balanced_df)
plt.title('Stroke Distribution')
plt.show()

# Calculate percentage of positive cases of stroke
total_cases = len(balanced_df)
positive_cases = balanced_df['stroke'].sum()
percentage_positive_cases = (positive_cases / total_cases) * 100
formatted_percentage = "{:.2f}".format(percentage_positive_cases)
print("Percentage of positive cases of stroke:", formatted_percentage, "%")

"""After sorting the dataset, only 5364 rows of data accepted and proceed to data modeling."""

print(len(balanced_df))  # or df.shape[0]

"""# Data Modeling

Split the dataset into training, validation and test sets

# Check the class distribution
"""

class_distribution = balanced_df['stroke'].value_counts()
print("Class Distribution:")
print(class_distribution)

"""# Split Original Dataset"""

X = data.drop(columns=['stroke'])
y = data['stroke']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42)

from sklearn.preprocessing import LabelEncoder

X_train_encoded = X_train.copy()
for col in X_train_encoded.columns:
    if X_train_encoded[col].dtype == 'object':
        le = LabelEncoder()
        X_train_encoded[col] = le.fit_transform(X_train_encoded[col])

X_test_encoded = X_test.copy()
for col in X_test_encoded.columns:
    if X_test_encoded[col].dtype == 'object':
        le = LabelEncoder()
        X_test_encoded[col] = le.fit_transform(X_test_encoded[col])

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_encoded, y_train)

print("Before SMOTE:", (y_train))
print("After SMOTE:", (y_train_smote))

print('               x       y ')
print('Original set:', X.shape, y.shape)
print('Training set:', X_train.shape, y_train.shape)
print('Test set:', X_test.shape, y_test.shape)

"""# Model Selection

###Model Training and Evaluation
"""

models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'AdaBoost': AdaBoostClassifier(),
}

print(X_train_smote.head())

for name, model in models.items():
    model.fit(X_train_smote, y_train_smote)
    predictions = model.predict(X_test_encoded)

    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)
    roc_auc = roc_auc_score(y_test, predictions)

    print(f"{name} - Acc: {accuracy:.3f}, Prec: {precision:.3f}, Rec: {recall:.3f}, F1: {f1:.3f}, AUC: {roc_auc:.3f}")

"""The Random Forest model stands out as the best choice for this classification task due to its superior F1 Score, indicating a balanced approach effectiveness in making correct predictions.

Thus, the Random Forest model should be chosen for deployment in this scenario.

Why use F1 Score? The F1 Score is the harmonic mean of precision and recall. It provides a balance between the two, which is particularly useful when need to take both false positives and false negatives into account.

Precision measures the proportion of true positive predictions among all positive predictions.

Recall measures the proportion of true positive predictions among all actual positive instances.

###Hyperparameter Tuning and Evaluation of Random Forest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    roc_auc_score, f1_score, confusion_matrix, ConfusionMatrixDisplay,
    make_scorer
)

# Step 1: 定义参数网格（适中范围）
param_grid = {
    'n_estimators': [100, 150, 200],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Step 2: 使用 recall 作为 scoring（因为我们关注正类识别）
recall_scorer = make_scorer(recall_score)

# Step 3: 网格搜索交叉验证
grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    scoring=recall_scorer,
    cv=3,
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_smote, y_train_smote)

# Step 4: 最佳模型
best_model = grid_search.best_estimator_
print("Best parameters:", grid_search.best_params_)

# Step 5: 在测试集上评估
best_predictions = best_model.predict(X_test)

# 评估指标
best_accuracy = accuracy_score(y_test, best_predictions)
best_precision = precision_score(y_test, best_predictions)
best_recall = recall_score(y_test, best_predictions)
best_roc_auc = roc_auc_score(y_test, best_predictions)
best_f1 = f1_score(y_test, best_predictions)

# Step 6: 输出结果
print("\nBest Model Metrics on Test Set:")
print(f"Accuracy : {best_accuracy:.4f}")
print(f"Precision: {best_precision:.4f}")
print(f"Recall   : {best_recall:.4f}")
print(f"F1 Score : {best_f1:.4f}")
print(f"ROC AUC  : {best_roc_auc:.4f}")

# Step 7: 混淆矩阵可视化
cm = confusion_matrix(y_test, best_predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap="Blues")

"""#Ensemble method/ model

Model Training and Evaluation with Advanced Boosting Algorithms
"""

# Initialize an empty list to store evaluation results
metrics_list = []
trained_models = {}

# Define the models
models = {
    'Gradient Boosting': GradientBoostingClassifier(),
    'AdaBoost': AdaBoostClassifier(),
}

best_model = None
best_f1_score = 0

# Train and evaluate models
for name, model in models.items():
    model.fit(X_train_smote, y_train_smote)
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)
    roc_auc = roc_auc_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)

    trained_models[name] = model

    # Store metrics in the list
    metrics_list.append({'Model': name,
                         'Accuracy': accuracy,
                         'Precision': precision,
                         'Recall': recall,
                         'F1 Score': f1})

    print(f"{name} Metrics:")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"ROC AUC: {roc_auc}")
    print(f"F1 Score: {f1}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, predictions)
    plt.figure()
    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    if f1 > best_f1_score:
        best_f1_score = f1
        best_model = model

print("Best Model:", best_model)
print("Best F1 Score on Test Set:", best_f1_score)

# Convert the list of dictionaries into a DataFrame
metrics_df = pd.DataFrame(metrics_list)

# Visualization of Metrics
plt.figure(figsize=(10, 6))
sns.barplot(data=metrics_df, x='Model', y='Accuracy', color='skyblue')
plt.title('Accuracy Comparison')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(data=metrics_df, x='Model', y='Precision', color='lightgreen')
plt.title('Precision Comparison')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(data=metrics_df, x='Model', y='Recall', color='lightcoral')
plt.title('Recall Comparison')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(data=metrics_df, x='Model', y='F1 Score', color='lightpink')
plt.title('F1 Score Comparison')
plt.xticks(rotation=45)
plt.show()

"""检查！！！The Gradient Boosting model is the best choice based on the highest F1 Score (0.8068). It strikes the best balance between precision and recall, making it suitable for tasks where both false positives and false negatives carry significant costs.

检查！！！In the context of healthcare, particularly for diagnosing diabetes, optimizing for recall ensures that fewer individuals with diabetes are missed. Missing a diagnosis can have serious health implications, including untreated high blood sugar levels, which can lead to complications such as heart disease, nerve damage, kidney failure, and even death.So below we will focus on optimize recall result for all the model.
"""

### Hyperparameter Tuning for AdaBoost Optimized for Recall

# Define the parameter grid for AdaBoost
ada_param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1, 1.0]
}

# Define custom scorer for recall
recall_scorer = make_scorer(recall_score)

# Perform grid search for recall
ada_grid_search_recall = GridSearchCV(estimator=AdaBoostClassifier(), param_grid=ada_param_grid, scoring=recall_scorer, cv=5)
ada_grid_search_recall.fit(X_train_smote, y_train_smote)

# Get the best AdaBoost model for recall
best_ada_model_recall = ada_grid_search_recall.best_estimator_

# Print the best parameters found for AdaBoost for recall
print("Best parameters for AdaBoost - Recall:", ada_grid_search_recall.best_params_)

# Predictions on test data for the best AdaBoost model
ada_predictions_recall = best_ada_model_recall.predict(X_test)

# Calculate evaluation metrics for the best AdaBoost model
ada_precision_recall = precision_score(y_test, ada_predictions_recall)
ada_recall_recall = recall_score(y_test, ada_predictions_recall)
ada_f1_recall = f1_score(y_test, ada_predictions_recall)

# Print scores for the best AdaBoost model
print("\nScores for AdaBoost after tuning - Recall:")
print(f"Precision: {ada_precision_recall}")
print(f"Recall: {ada_recall_recall}")
print(f"F1 Score: {ada_f1_recall}")

"""##Hyperparameter Tuning for Gradient Boosting Optimized for Recall"""

# Define the parameter grid for Gradient Boosting
gb_param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 5]
}

# Define custom scorer for recall
recall_scorer = make_scorer(recall_score)

# Perform grid search for recall
gb_grid_search_recall = GridSearchCV(estimator=GradientBoostingClassifier(), param_grid=gb_param_grid, scoring=recall_scorer, cv=5)
gb_grid_search_recall.fit(X_train_smote, y_train_smote)

# Get the best Gradient Boosting model for recall
best_gb_model_recall = gb_grid_search_recall.best_estimator_

# Print the best parameters found for Gradient Boosting for recall
print("Best parameters for Gradient Boosting - Recall:", gb_grid_search_recall.best_params_)

# Predictions on test data for the best Gradient Boosting model
gb_predictions_recall = best_gb_model_recall.predict(X_test)

# Calculate evaluation metrics for the best Gradient Boosting model
gb_precision_recall = precision_score(y_test, gb_predictions_recall)
gb_recall_recall = recall_score(y_test, gb_predictions_recall)
gb_f1_recall = f1_score(y_test, gb_predictions_recall)

# Print scores for the best Gradient Boosting model
print("\nScores for Gradient Boosting after tuning - Recall:")
print(f"Precision: {gb_precision_recall}")
print(f"Recall: {gb_recall_recall}")
print(f"F1 Score: {gb_f1_recall}")

"""# Cross-Validation Evaluation of Stacking Ensemble Model for Diabetes Detection"""

# Perform cross-validation for AdaBoost
ada_cv_scores = cross_val_score(best_ada_model_recall, X_train_smote, y_train_smote, cv=5, scoring='f1')

# Print cross-validation scores for AdaBoost
print("Cross-Validation Scores for AdaBoost:")
print(ada_cv_scores)

# Calculate mean and standard deviation of cross-validation scores for AdaBoost
ada_cv_mean_score = np.mean(ada_cv_scores)
ada_cv_std_score = np.std(ada_cv_scores)

print("\nMean Cross-Validation Score for AdaBoost:", ada_cv_mean_score)
print("Standard Deviation of Cross-Validation Scores for AdaBoost:", ada_cv_std_score)

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# Define the stacking classifier with AdaBoost and Random Forest as base estimators
estimators = [
    ('ada', best_ada_model_recall),
    ('rf', best_model)
]

# Initialize the stacking classifier with Logistic Regression as the final estimator
stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())

# Fit the stacking classifier on the resampled training data
stacking_clf.fit(X_train_smote, y_train_smote)

# Predictions on test data
stacking_predictions = stacking_clf.predict(X_test)

# Calculate evaluation metrics
stacking_accuracy = accuracy_score(y_test, stacking_predictions)
stacking_precision = precision_score(y_test, stacking_predictions)
stacking_recall = recall_score(y_test, stacking_predictions)
stacking_roc_auc = roc_auc_score(y_test, stacking_predictions)
stacking_f1 = f1_score(y_test, stacking_predictions)

# Print scores for the stacking classifier
print("\nScores for Stacking Classifier:")
print(f"Accuracy: {stacking_accuracy}")
print(f"Precision: {stacking_precision}")
print(f"Recall: {stacking_recall}")
print(f"ROC AUC: {stacking_roc_auc}")
print(f"F1 Score: {stacking_f1}")

"""检查！！！The Random Forest Regression model was trained and tuned using a combination of hyperparameters: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 100}. After hyperparameter tuning, the model achieved a validation accuracy of approximately 96.34%, indicating its ability to generalize well to unseen data.

When evaluated on the test set, the model demonstrated a high accuracy of approximately 96.20%, further confirming its robustness and effectiveness in making accurate predictions.

In terms of regression performance metrics, the model yielded a Root Mean Squared Error (RMSE) of approximately 0.195 and a Residual Standard Error (RSE) of approximately 0.195. These metrics suggest that, on average, the model's predictions are about 0.195 units away from the actual values, with a standard deviation of 0.195. Additionally, the R-squared (R2 Score) value of approximately 0.627 indicates that the model explains about 62.7% of the variance in the dependent variable.

Overall, the Random Forest Regression model with the tuned hyperparameters demonstrates strong performance in predicting the target variable. However, there may still be some room for improvement, especially in reducing the RMSE and RSE values further to enhance the model's predictive accuracy. Further exploration of feature engineering, model selection, or additional hyperparameter tuning could potentially lead to even better performance.
"""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score

# Define the number of folds for cross-validation
num_folds = 5

# Initialize Stratified K-Fold cross-validator
kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

# Perform cross-validation for the stacking classifier with accuracy
stacking_cv_scores_accuracy = cross_val_score(stacking_clf, X_train_smote, y_train_smote, cv=kfold, scoring='accuracy')

# Print the cross-validation accuracy scores
print("\nCross-validation accuracy scores for Stacking Classifier:")
for i, score in enumerate(stacking_cv_scores_accuracy, start=1):
    print(f"Fold {i}: {score}")

# Calculate and print the mean and standard deviation of cross-validation accuracy scores
mean_cv_score_accuracy = stacking_cv_scores_accuracy.mean()
std_cv_score_accuracy = stacking_cv_scores_accuracy.std()
print("\nMean cross-validation accuracy score:", mean_cv_score_accuracy)
print("Standard deviation of cross-validation accuracy scores:", std_cv_score_accuracy)

# Perform cross-validation for the stacking classifier with F1 score
stacking_cv_scores_f1 = cross_val_score(stacking_clf, X_train_smote, y_train_smote, cv=kfold, scoring='f1')

# Print the cross-validation F1 scores
print("\nCross-validation F1 scores for Stacking Classifier:")
for i, score in enumerate(stacking_cv_scores_f1, start=1):
    print(f"Fold {i}: {score}")

# Calculate and print the mean and standard deviation of cross-validation F1 scores
mean_cv_score_f1 = stacking_cv_scores_f1.mean()
std_cv_score_f1 = stacking_cv_scores_f1.std()
print("\nMean cross-validation F1 score:", mean_cv_score_f1)
print("Standard deviation of cross-validation F1 scores:", std_cv_score_f1)

import joblib

# Define the filename for saving the model
filename = 'stacking_classifier_model.pkl'

# Save the model to a file
joblib.dump(stacking_clf, filename)

print("Stacking classifier model saved to", filename)

import joblib

# Define filenames for saving the models
ada_model_filename = 'best_ada_model_recall.pkl'
rf_model_filename = 'best_rf_model.pkl'

# Save the AdaBoost model to a file
joblib.dump(best_ada_model_recall, ada_model_filename)
print("AdaBoost model saved to", ada_model_filename)

# Save the Random Forest model to a file
joblib.dump(best_model, rf_model_filename)
print("Random Forest model saved to", rf_model_filename)

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Calculate RMSE
test_predictions = stacking_clf.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, test_predictions))

# Calculate RSE
n = len(y_test)
p = X_test.shape[1]  # Number of predictors/features
rse = np.sqrt((1 / (n - p - 1)) * np.sum((y_test - test_predictions) ** 2))

# Calculate R2 Score
r2 = r2_score(y_test, test_predictions)


# # Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test, test_predictions))

# # Calculate RSE
n = len(y_test)
p = X_test.shape[1]  # Number of predictors/features
rse = np.sqrt((1 / (n - p - 1)) * np.sum((y_test - test_predictions) ** 2))

# # Calculate R2 Score
r2 = r2_score(y_test, test_predictions)

# # Print the evaluation metrics
print("RMSE:", rmse)
print("RSE:", rse)
print("R-squared (R2 Score):", r2)